{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef949e-51bd-438b-9803-d58970cfd6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from delphi.networks.ConvNets import BrainStateClassifier3d\n",
    "from delphi.utils.datasets import NiftiDataset\n",
    "from delphi.utils.tools import ToTensor, compute_accuracy, convert_wandb_config, read_config\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59b2d2-f6c5-425c-836b-7e596e392fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    import random\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    g = torch.Generator()  # can be used in pytorch dataloaders for reproducible sample selection when shuffle=True\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    return g\n",
    "\n",
    "g = set_random_seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b01b8e-62ca-4ea9-bd01-34c6444f75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_plots(y_true, y_pred, y_prob, class_labels, dataset):\n",
    "    wandb.log({\n",
    "        f\"{dataset}-ROC\": wandb.plot.roc_curve(y_true=y_true, y_probas=y_prob, labels=class_labels),\n",
    "        f\"{dataset}-PR\": wandb.plot.pr_curve(y_true=y_true, y_probas=y_prob, labels=class_labels, ),\n",
    "        f\"{dataset}-ConfMat\": wandb.plot.confusion_matrix(y_true=y_true, preds=y_pred, class_names=class_labels)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3121c-e2da-4b4a-b20a-01c76f0b4604",
   "metadata": {},
   "source": [
    "# Define the classes and data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b653d2-d91b-4c98-9e1b-d14ed5ceb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = sorted([\"handleft\", \"handright\", \"footleft\", \"footright\", \"tongue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d2540-9c71-4c0b-aaea-e50d5c7792ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = NiftiDataset(\"../t-maps/test\", class_labels, 0, device=DEVICE, transform=ToTensor())\n",
    "\n",
    "# we will split the train dataset into a train (80%) and validation (20%) set.\n",
    "data_train_full = NiftiDataset(\"../t-maps/train\", class_labels, 10, device=DEVICE, transform=ToTensor())\n",
    "\n",
    "# we want one stratified shuffled split\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=2020)\n",
    "idx_train, idx_valid = next(sss.split(data_train_full.data, data_train_full.labels))\n",
    "\n",
    "data_train = torch.utils.data.Subset(data_train_full, idx_train)\n",
    "data_valid = torch.utils.data.Subset(data_train_full, idx_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330748c0-72f0-4748-b7be-eeee278d0904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "459a3075-55b9-46a1-81c7-7fbd2d3646ba",
   "metadata": {},
   "source": [
    "# Set up the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52c0ce-26e1-4eb9-81d9-e5266ea7e9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38759e91-e10c-47cc-a639-98d03e581869",
   "metadata": {},
   "source": [
    "# Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97713ef2-507d-4776-a0b9-986d123502d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(model, config, save_name, logwandb=True):\n",
    "    \n",
    "    dl_test = DataLoader(data_test, batch_size=config.batch_size, shuffle=True, generator=g)\n",
    "    dl_train = DataLoader(data_train, batch_size=config.batch_size, shuffle=True, generator=g)\n",
    "    dl_valid = DataLoader(data_valid, batch_size=config.batch_size, shuffle=True, generator=g)\n",
    "    \n",
    "    best_loss, best_acc = 100, 0\n",
    "    loss_acc = []\n",
    "    train_stats, valid_stats = [], []\n",
    "    patience = 1\n",
    "    patience_ctr = 0\n",
    "    \n",
    "    # loop for the above set number of epochs\n",
    "    for epoch in range(0, config.epochs):\n",
    "        _, _ = model.fit(dl_train, lr=config.learning_rate, device=DEVICE)\n",
    "\n",
    "        # for validating or testing set the network into evaluation mode such that layers like dropout are not active\n",
    "        with torch.no_grad():\n",
    "            tloss, tstats = model.fit(dl_train, device=DEVICE, train=False)\n",
    "            vloss, vstats = model.fit(dl_valid, device=DEVICE, train=False)\n",
    "                    \n",
    "        tacc = compute_accuracy(tstats[:, -2], tstats[:, -1])\n",
    "        vacc = compute_accuracy(vstats[:, -2], vstats[:, -1])\n",
    "\n",
    "        loss_acc.append(pd.DataFrame([[tloss, vloss, tacc, vacc]],\n",
    "                                     columns=[\"train_loss\", \"valid_loss\", \"train_acc\", \"valid_acc\"]))\n",
    "        \n",
    "        train_stats.append(pd.DataFrame(tstats.tolist(), columns=[*class_labels, *[\"real\", \"predicted\"]]))\n",
    "        train_stats[epoch][\"epoch\"] = epoch\n",
    "        valid_stats.append(pd.DataFrame(vstats.tolist(), columns=[*class_labels, *[\"real\", \"predicted\"]]))\n",
    "        valid_stats[epoch][\"epoch\"] = epoch\n",
    "        \n",
    "        wandb.log({\n",
    "            \"train_acc\": tacc, \"train_loss\": tloss,\n",
    "            \"valid_acc\": vacc, \"valid_loss\": vloss\n",
    "        })\n",
    "        \n",
    "        print('Epoch=%03d, train_loss=%2.3f, train_acc=%1.3f, valid_loss=%2.3f, valid_acc=%1.3f' % \n",
    "             (epoch, tloss, tacc, vloss, vacc))\n",
    "        \n",
    "        if (vacc >= best_acc) and (vloss <= best_loss):\n",
    "            # assign the new best values\n",
    "            best_acc, best_loss = vacc, vloss\n",
    "            wandb.run.summary[\"best_valid_accuracy\"] = best_acc\n",
    "            wandb.run.summary[\"best_valid_epoch\"] = epoch\n",
    "            # save the current best model\n",
    "            model.save(save_name)\n",
    "            # plot some graphs for the validation data\n",
    "            wandb_plots(vstats[:, -2], vstats[:, -1], vstats[:, :-2], class_labels, \"valid\")\n",
    "        else:\n",
    "            patience_ctr+=1\n",
    "        \n",
    "        if patience_ctr > patience:\n",
    "            print('Reached patience. Stopping training and continuing with test set.')\n",
    "            break\n",
    "\n",
    "    # save the files\n",
    "    full_df = pd.concat(loss_acc)\n",
    "    full_df.to_csv(os.path.join(save_name, \"loss_acc_curves.csv\"), index=False)\n",
    "    full_df = pd.concat(train_stats)\n",
    "    full_df.to_csv(os.path.join(save_name, \"train_stats.csv\"), index=False)\n",
    "    full_df = pd.concat(valid_stats)\n",
    "    full_df.to_csv(os.path.join(save_name, \"valid_stats.csv\"), index=False)\n",
    "    \n",
    "    # EVALUATE THE MODEL ON THE TEST DATA\n",
    "    with torch.no_grad():\n",
    "        testloss, teststats = model.fit(dl_test, train=False)\n",
    "    testacc = compute_accuracy(teststats[:, -2], teststats[:, -1])\n",
    "    wandb.run.summary[\"test_accuracy\"] = testacc\n",
    "\n",
    "    wandb.log({\"test_accuraccy\": testacc, \"test_loss\": testloss})\n",
    "    wandb_plots(teststats[:, -2], teststats[:, -1], teststats[:, :-2], class_labels, \"test\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e36224-393e-483d-b4a5-c08cde65d39e",
   "metadata": {},
   "source": [
    "# Define the run_train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e68c98-e95d-4c3e-8018-1a098091139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training function with the wandb init\n",
    "def run_train():\n",
    "    \n",
    "    # here we initialize weights&biases. \n",
    "    with wandb.init() as run:\n",
    "        # here's the promised conversion of the wandb.config\n",
    "        # this results into a dict that contains key-value pairs that we can use to configure our network:\n",
    "        # converted_config['lin_neurons'] = [512, 8, 128]\n",
    "                \n",
    "        converted_config = convert_wandb_config(wandb.config, BrainStateClassifier3d._REQUIRED_PARAMS)\n",
    "                \n",
    "        model = BrainStateClassifier3d((91, 109, 91), len(class_labels), converted_config)\n",
    "        \n",
    "        # We do not necessarily need this line but it is nice to update the config.\n",
    "        #wandb.config.update(model.config, allow_val_change=True)\n",
    "        \n",
    "        t_stamp = time.time()\n",
    "        save_name = os.path.join(\"models\", f\"motor-explo_{t_stamp}\")\n",
    "        wandb.run.name = f\"motor-explo-{t_stamp}\"\n",
    "        \n",
    "        # now train the netwok, yay!\n",
    "        train_net(model, wandb.config, save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238daa7-5ceb-4add-9ded-4ec9d76511d4",
   "metadata": {},
   "source": [
    "# Run the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8f830-d28c-4ef1-a24a-91c1cbd462c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = read_config(\"exploration_sweep.yaml\")\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d5adb-7b16-4081-9ae2-eda6dd19333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wandb sweep config\n",
    "#os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_ENTITY'] = \"philis893\" # this is my wandb account name. This can also be a group name, for example\n",
    "os.environ['WANDB_PROJECT'] = \"thesis\" # this is simply the project name where we want to store the sweep logs and plots\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee61b9-76f4-484b-87a7-5b55732f5a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count=10\n",
    "wandb.agent(sweep_id, function=run_train, count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00948e00-9286-46e9-aa79-45cf64ab5b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51fe2a-de78-48a3-9243-8799ceed1f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
