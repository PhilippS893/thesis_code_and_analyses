{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199f3688-c4a4-4652-865c-740ab265e6aa",
   "metadata": {},
   "source": [
    "# Train and evaluate a 3D Convolutional Neural Network (3dCNN) to classify motor tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512daf9-457e-4fb1-8f7d-b02a17edddca",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>In this notebook we create and train a 3D-Convolutional Neural Network which learns to classify different patterns of whole-brain fMRI statistical parameters (t-scores). In this first approach our goal is to train a classifier that can reliably distinguish between such whole-brain patterns for five limb movements (i.e., left/right hand, left/right foot, and tongue).\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed4397e-7b43-4598-9e27-a6f09fdc2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d87eac6-0be1-4c62-b097-753fc82f131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import os, wandb, torch, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from delphi import mni_template\n",
    "from delphi.networks.ConvNets import BrainStateClassifier3d\n",
    "from delphi.utils.datasets import NiftiDataset\n",
    "from delphi.utils.tools import ToTensor, compute_accuracy, convert_wandb_config, read_config, z_transform_volume\n",
    "from delphi.utils.plots import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# you can find all these files in ../utils\n",
    "from utils.tools import attribute_with_method, concat_stat_files, compute_mi\n",
    "from utils.wandb_funcs import reset_wandb_env, wandb_plots\n",
    "from utils.random import set_random_seed\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b2335-28be-41c8-addf-9b9788f39cb2",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>To make sure that we obtain (almost) the same results for each execution we set the random seed of multiple different librabries (i.e., torch, random, numpy)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5ca42e-df0c-46ef-a1cf-771b1fa3d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = set_random_seed(2020) # the project started in the year 2020, hence the seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df7fee-a2ef-47fc-a91f-b28d6670b434",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ca8fb-a9c1-424a-beeb-c91ddfd1b0be",
   "metadata": {},
   "source": [
    "In this section, we define and initialize our required variables. We first need to define which classes we want to predict, i.e., the conditions of the motor mapper. We then define a PyTorch dataset; in this case `NiftiDataset` is a custom written Dataset-Class (see https://github.com/PhilippS893/delphi). As is common practice in machine learning projects, we split our data into a training and validation dataset (ratio=80 to 20, respectively).\n",
    "\n",
    "Note: In case it is necessary to create a null-model, i.e., a neural network that is trained on data where the labels are randomized, one can set the parameter `shuffe_labels=False` to `True`. This is usually done to have a baseline for the null hypothesis that \"everything is random\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acec5605-27f4-452f-88d1-07a0f54d69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_LABEL = \"motor\"\n",
    "class_labels = sorted([\"handleft\", \"handright\", \"footleft\", \"footright\", \"tongue\"])\n",
    "\n",
    "data_test = NiftiDataset(\"../t-maps/test\", class_labels, 0, device=DEVICE, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34364974-90ec-4926-b384-297fd4b0a1b5",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>We now set some parameters required by w&b to properly store information about our trained neural networks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c0b297-b18d-4e6d-b965-8f32551c649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wandb sweep config\n",
    "# os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_ENTITY'] = \"philis893\" # this is my wandb account name. This can also be a group name, for example\n",
    "os.environ['WANDB_PROJECT'] = \"thesis\" # this is simply the project name where we want to store the sweep logs and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69bd8c8-3f32-4293-8cb9-8ef47b87d297",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training the neural network(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8371a07-a271-4b50-8812-5e6c26673735",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>In this first approach, we estimated what parameters we could use for our 3d-CNN from existing literature. We make use of some functionality, e.g., the function \"read_config\", I wrote for the \"delphi\" toolbox (see https://github.com/PhilippS893/delphi). This function can read .yaml files with the formatting as below:</p>\n",
    "\n",
    "The contents of `hyperparameter.yaml`: <br>\n",
    "`kernel_size: 7`<br>\n",
    "`batch_size: 4`<br>\n",
    "`dropout: .5`<br>\n",
    "`learning_rate: 0.00001`<br>\n",
    "`epochs: 60`<br>\n",
    "`channels: [1, 8, 16, 32, 64]`<br>\n",
    "`lin_neurons: [128, 64]`<br>\n",
    "`pooling_kernel: 2`<br>\n",
    "\n",
    "The function `read_config` will return a dictionary variable containing all keyword-value pairs as set in the `hyperparameter.yaml`. We do this, because the `delphi` toolbox was written with dictionaries as configuration variables in mind and because we can easily submit dictionaries to `w&b` to keep track of such parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b5777-3c39-47b0-9c5d-58cc31d75169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(run, fold, data_train, data_valid, data_test, hp, class_labels, shuffled_labels=False):\n",
    "    reset_wandb_env()\n",
    "    \n",
    "    job_name = \"CV-shuffled\" if shuffled_labels else \"CV-real\"\n",
    "    wandb_kwargs = {\n",
    "        \"entity\": os.environ['WANDB_ENTITY'],\n",
    "        \"project\": os.environ['WANDB_PROJECT'],\n",
    "        \"group\": \"first-steps\",\n",
    "        \"name\": f\"motor_fold-{fold:02d}\",\n",
    "        \"job_type\": job_name if num_folds > 1 else \"train\",\n",
    "    }\n",
    "    \n",
    "    save_name = os.path.join(\"models\", job_name, wandb_kwargs[\"name\"])\n",
    "    if os.path.exists(save_name):\n",
    "        return\n",
    "    \n",
    "    # we adjust the random seed here to ensure that each run of each fold has a unique seed!\n",
    "    g = set_random_seed(2020 + fold + run)\n",
    "    \n",
    "    # we now use the wandb context to track the training and evaluation process.\n",
    "    # all settings and changes will be reset at the beginning of the fold-loop. (see line 11)\n",
    "    with wandb.init(config=hp, **wandb_kwargs) as run:\n",
    "        \n",
    "        # please note that this conversion is unnecessary if not using w&b!\n",
    "        model_cfg = convert_wandb_config(run.config, BrainStateClassifier3d._REQUIRED_PARAMS)\n",
    "        \n",
    "        # setup a model with the parameters given in model_cfg\n",
    "        model = BrainStateClassifier3d(input_dims, len(class_labels), model_cfg)\n",
    "        model.to(DEVICE);\n",
    "        \n",
    "        model.config[\"class_labels\"] = class_labels\n",
    "        \n",
    "        dl_train = DataLoader(data_train, batch_size=run.config.batch_size, shuffle=True, generator=g)\n",
    "        dl_valid = DataLoader(data_valid, batch_size=run.config.batch_size, shuffle=True, generator=g)\n",
    "        dl_test = DataLoader(data_test, batch_size=run.config.batch_size, shuffle=False, generator=g)\n",
    "\n",
    "        best_loss, best_acc = 100, 0\n",
    "        loss_acc = []\n",
    "        train_stats, valid_stats = [], []\n",
    "\n",
    "        # loop for the above set number of epochs\n",
    "        for epoch in range(run.config.epochs):\n",
    "            _, _ = model.fit(dl_train, lr=run.config.learning_rate)\n",
    "\n",
    "            # for validating or testing set the network into evaluation mode such that layers like dropout are not active\n",
    "            with torch.no_grad():\n",
    "                tloss, tstats = model.fit(dl_train, train=False)\n",
    "                vloss, vstats = model.fit(dl_valid, train=False)\n",
    "\n",
    "            # the model.fit() method has 2 output parameters: loss, stats = model.fit()\n",
    "            # the first parameter is simply the loss for each sample\n",
    "            # the second parameter is a matrix of n_classes+2-by-n_samples\n",
    "            # the first n_classes columns are the output probabilities of the model per class\n",
    "            # the second to last column (i.e., [:, -2]) represents the real labels\n",
    "            # the last column (i.e., [:, -1]) represents the predicted labels\n",
    "            tacc = compute_accuracy(tstats[:, -2], tstats[:, -1])\n",
    "            vacc = compute_accuracy(vstats[:, -2], vstats[:, -1])\n",
    "\n",
    "            loss_acc.append(pd.DataFrame([[tloss, vloss, tacc, vacc]],\n",
    "                                         columns=[\"train_loss\", \"valid_loss\", \"train_acc\", \"valid_acc\"]))\n",
    "\n",
    "            train_stats.append(pd.DataFrame(tstats.tolist(), columns=[*class_labels, *[\"real\", \"predicted\"]]))\n",
    "            train_stats[epoch][\"epoch\"] = epoch\n",
    "            valid_stats.append(pd.DataFrame(vstats.tolist(), columns=[*class_labels, *[\"real\", \"predicted\"]]))\n",
    "            valid_stats[epoch][\"epoch\"] = epoch\n",
    "\n",
    "            wandb.log({\n",
    "                \"train_acc\": tacc, \"train_loss\": tloss,\n",
    "                \"valid_acc\": vacc, \"valid_loss\": vloss\n",
    "            }, step=epoch)\n",
    "\n",
    "            print('Epoch=%03d, train_loss=%2.3f, train_acc=%1.3f, valid_loss=%2.3f, valid_acc=%1.3f' % \n",
    "                 (epoch, tloss, tacc, vloss, vacc))\n",
    "\n",
    "            if (vacc >= best_acc) and (vloss <= best_loss):\n",
    "                # assign the new best values\n",
    "                best_acc, best_loss = vacc, vloss\n",
    "                wandb.run.summary[\"best_valid_accuracy\"] = best_acc\n",
    "                wandb.run.summary[\"best_valid_epoch\"] = epoch\n",
    "                # save the current best model\n",
    "                model.save(save_name)\n",
    "                # plot some graphs for the validation data\n",
    "                wandb_plots(vstats[:, -2], vstats[:, -1], vstats[:, :-2], class_labels, \"valid\")\n",
    "\n",
    "\n",
    "        # save the files\n",
    "        full_df = pd.concat(loss_acc)\n",
    "        full_df.to_csv(os.path.join(save_name, \"loss_acc_curves.csv\"), index=False)\n",
    "        full_df = pd.concat(train_stats)\n",
    "        full_df.to_csv(os.path.join(save_name, \"train_stats.csv\"), index=False)\n",
    "        full_df = pd.concat(valid_stats)\n",
    "        full_df.to_csv(os.path.join(save_name, \"valid_stats.csv\"), index=False)\n",
    "\n",
    "        # load the best performing model\n",
    "        model = BrainStateClassifier3d(save_name)\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        # EVALUATE THE MODEL ON THE TEST DATA\n",
    "        with torch.no_grad():\n",
    "            testloss, teststats = model.fit(dl_test, train=False)\n",
    "            \n",
    "        testacc = compute_accuracy(teststats[:, -2], teststats[:, -1])\n",
    "        df_test = pd.DataFrame(teststats.tolist(), columns=[*class_labels, *[\"real\", \"predicted\"]])\n",
    "        df_test.to_csv(os.path.join(save_name, \"test_stats.csv\"), index=False)\n",
    "        \n",
    "        wandb.run.summary[\"test_accuracy\"] = testacc\n",
    "\n",
    "        wandb.log({\"test_accuracy\": testacc, \"test_loss\": testloss})\n",
    "        wandb_plots(teststats[:, -2], teststats[:, -1], teststats[:, :-2], class_labels, \"test\")\n",
    "\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c595940-a263-4257-95be-38f895269e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = read_config(\"hyperparameter.yaml\")\n",
    "\n",
    "num_folds = 10 # we decided to run a 10-fold cross-validation scheme\n",
    "run = 0 # in case you decide to do multiple runs of the cross-validation scheme\n",
    "input_dims = (91, 109, 91) # these are the 3D input dimension of our whole-brain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40287c-57a6-43f3-a812-38b6ecea3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f88ea-db23-4fc6-a4a3-b59243f2a4fd",
   "metadata": {},
   "source": [
    "### Run cross-validation training using correct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd352b-3993-4986-8d8c-425dda5ecae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will split the train dataset into a train (80%) and validation (20%) set.\n",
    "data_train_full = NiftiDataset(\"../t-maps/train\", class_labels, 0, device=DEVICE, \n",
    "                               transform=ToTensor(), shuffle_labels=False)\n",
    "\n",
    "# we want a stratified shuffled split\n",
    "sss = StratifiedShuffleSplit(n_splits=num_folds, test_size=0.2, random_state=2020)\n",
    "\n",
    "for fold, (idx_train, idx_valid) in enumerate(sss.split(data_train_full.data, data_train_full.labels)):\n",
    "    data_train = torch.utils.data.Subset(data_train_full, idx_train)\n",
    "    data_valid = torch.utils.data.Subset(data_train_full, idx_valid)\n",
    "    train_network(run, fold, data_train, data_valid, data_test, hp, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e6f2f-8ae9-4b21-b11b-5085418d7f25",
   "metadata": {},
   "source": [
    "### Run cross-validation training using shuffled labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b378f-67ee-4936-a219-e3a419acb8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will split the train dataset into a train (80%) and validation (20%) set.\n",
    "data_train_full = NiftiDataset(\"../t-maps/train\", class_labels, 20, device=DEVICE, \n",
    "                               transform=ToTensor(), shuffle_labels=True)\n",
    "\n",
    "# we want a stratified shuffled split\n",
    "sss = StratifiedShuffleSplit(n_splits=num_folds, test_size=0.2, random_state=2020)\n",
    "\n",
    "for fold, (idx_train, idx_valid) in enumerate(sss.split(data_train_full.data, data_train_full.labels)):\n",
    "    data_train = torch.utils.data.Subset(data_train_full, idx_train)\n",
    "    data_valid = torch.utils.data.Subset(data_train_full, idx_valid)\n",
    "    train_network(run, fold, data_train, data_valid, data_test, hp, class_labels, shuffled_labels=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
